services:
  
  # MLflow Tracking Server
  mlflow-server:
    image: ${MLFLOW_IMAGE:-ghcr.io/mlflow/mlflow:v3.8.0}
    container_name: ${MLFLOW_CONTAINER_NAME:-mlflow-tracking-server}
    ports:
      - "${MLFLOW_PORT:-5112}:${MLFLOW_PORT:-5112}"
    command: >
      mlflow server
      --host ${MLFLOW_HOST:-0.0.0.0}
      --port ${MLFLOW_PORT:-5112}
    networks:
      - ${ML_NETWORK_NAME:-ml-network}
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:${MLFLOW_PORT:-5112}/health" ]
      interval: ${MLFLOW_HEALTH_INTERVAL:-30s}
      timeout: ${MLFLOW_HEALTH_TIMEOUT:-10s}
      retries: ${MLFLOW_HEALTH_RETRIES:-5}

  
  # PostgreSQL
  postgres:
    image: ${POSTGRES_IMAGE:-postgres:15-alpine}
    container_name: ${POSTGRES_CONTAINER_NAME:-postgres}
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-admin}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-admin}
      POSTGRES_DB: ${POSTGRES_DB:-app}
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - ${POSTGRES_VOLUME_NAME:-postgres-data}:/var/lib/postgresql/data
    networks:
      - ${ML_NETWORK_NAME:-ml-network}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-admin}"]
      interval: ${POSTGRES_HEALTH_INTERVAL:-10s}
      timeout: ${POSTGRES_HEALTH_TIMEOUT:-5s}
      retries: ${POSTGRES_HEALTH_RETRIES:-5}

  
  # Airflow Init
  airflow-init:
    build:
      context: ${AIRFLOW_BUILD_CONTEXT:-.}
      dockerfile: ${AIRFLOW_DOCKERFILE:-airflow.Dockerfile}
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR:-LocalExecutor}
      AIRFLOW__CORE__DAGS_FOLDER: ${AIRFLOW_DAGS_FOLDER:-/src/airflow}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW_DB_CONN}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW_LOAD_EXAMPLES:-"false"}
      AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: ${AIRFLOW_LOAD_DEFAULT_CONNECTIONS:-"true"}
    depends_on:
      - postgres
    volumes:
      - ${PROJECT_SRC_VOLUME:-./src}:/src
      - ${AIRFLOW_VOLUME_NAME:-airflow-db}:/opt/airflow
    command: >
      bash -c "
            airflow db migrate &&
            airflow connections create-default-connections
      "
    networks:
      - ${ML_NETWORK_NAME:-ml-network}

  
  # Airflow Webserver
  airflow-server:
    build:
      context: ${AIRFLOW_BUILD_CONTEXT:-.}
      dockerfile: ${AIRFLOW_DOCKERFILE:-airflow.Dockerfile}
    container_name: ${AIRFLOW_WEBSERVER_CONTAINER:-airflow-webserver}
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR:-LocalExecutor}
      AIRFLOW__CORE__DAGS_FOLDER: ${AIRFLOW_DAGS_FOLDER:-/src/airflow}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW_DB_CONN}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW_LOAD_EXAMPLES:-"false"}
    volumes:
      - ${PROJECT_SRC_VOLUME:-./src}:/src
      - ${AIRFLOW_VOLUME_NAME:-airflow-db}:/opt/airflow
    ports:
      - "${AIRFLOW_WEB_PORT:-8823}:8080"
    command: ${AIRFLOW_WEBSERVER_COMMAND:-airflow standalone}
    networks:
      - ${ML_NETWORK_NAME:-ml-network}
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:8080/api/v2/monitor/health" ]
      interval: ${AIRFLOW_HEALTH_INTERVAL:-30s}
      timeout: ${AIRFLOW_HEALTH_TIMEOUT:-10s}
      retries: ${AIRFLOW_HEALTH_RETRIES:-5}
      start_period: ${AIRFLOW_HEALTH_START_PERIOD:-30s}

  
  # Iceberg REST Catalog
  iceberg-rest:
    image: ${ICEBERG_REST_IMAGE:-tabulario/iceberg-rest:0.6.0}
    container_name: ${ICEBERG_REST_CONTAINER:-iceberg-ml-catalog}
    ports:
      - "${ICEBERG_REST_PORT:-8181}:8181"
    environment:
      CATALOG_WAREHOUSE: ${ICEBERG_WAREHOUSE}
      CATALOG_IO__IMPL: ${ICEBERG_IO_IMPL:-org.apache.iceberg.aws.s3.S3FileIO}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_REGION: ${AWS_REGION}
      AWS_DEFAULT_REGION: ${AWS_REGION}
    networks:
      - ${ML_NETWORK_NAME:-ml-network}

  
  # Spark Master
  spark-master:
    image: ${SPARK_IMAGE:-apache/spark:3.5.0}
    container_name: ${SPARK_MASTER_CONTAINER:-spark-master}
    environment:
      SPARK_MODE: master
      SPARK_RPC_AUTHENTICATION_ENABLED: ${SPARK_RPC_AUTH:-no}
      SPARK_RPC_ENCRYPTION_ENABLED: ${SPARK_RPC_ENCRYPTION:-no}
      SPARK_NO_DAEMONIZE: ${SPARK_NO_DAEMONIZE:-true}
    ports:
      - "${SPARK_UI_PORT:-8080}:8080"
      - "${SPARK_MASTER_PORT:-7077}:7077"
    command: [ "/opt/spark/sbin/start-master.sh" ]
    networks:
      - ${ML_NETWORK_NAME:-ml-network}
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080" ]
      interval: ${SPARK_HEALTH_INTERVAL:-30s}
      timeout: ${SPARK_HEALTH_TIMEOUT:-10s}
      retries: ${SPARK_HEALTH_RETRIES:-5}

  
  # Spark Worker
  spark-worker:
    image: ${SPARK_IMAGE:-apache/spark:3.5.0}
    container_name: ${SPARK_WORKER_CONTAINER:-spark-worker}
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: ${SPARK_MASTER_URL:-spark://spark-master:7077}
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY:-4G}
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES:-4}
      SPARK_NO_DAEMONIZE: ${SPARK_NO_DAEMONIZE:-true}
    depends_on:
      spark-master:
        condition: service_healthy
    command: [ "/opt/spark/sbin/start-worker.sh", "${SPARK_MASTER_URL:-spark://spark-master:7077}" ]
    networks:
      - ${ML_NETWORK_NAME:-ml-network}

  
  # Model Serving API
  model-server:
    build:
      context: ${MODEL_SERVER_BUILD_CONTEXT:-.}
      dockerfile: ${MODEL_SERVER_DOCKERFILE:-server.Dockerfile}
    container_name: ${MODEL_SERVER_CONTAINER:-model-serving-api}
    ports:
      - "${MODEL_SERVER_PORT:-8000}:8000"
    environment:
      MLFLOW_TRACKING_URI: ${MODEL_MLFLOW_TRACKING_URI}
      MODEL_NAME: ${MODEL_NAME}
      MODEL_STAGE: ${MODEL_STAGE}
    depends_on:
      - mlflow-server
    networks:
      - ${ML_NETWORK_NAME:-ml-network}


# Networks
networks:
  ml-network:
    driver: ${ML_NETWORK_DRIVER:-bridge}


# Volumes
volumes:
  postgres-data:
  airflow-db:
